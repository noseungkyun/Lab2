******************************************* 첫번째 시험
1. To input a file into HDFS, the client app passes the data to NameNode, 
Which then divides the data into blocks to the DataNodes.
>>>>> false

2. What component (Master node) of HDFS is responsible for maintaining the namespace of the distributed file system?
>>>>> Name Node

3. What is the default file replication factor in HDFS?
>>>>> 3

4. The NameNode maintains the namespace of the file system using which two sets of files?
>>>>> edits, fsimage

5. Which type of Hadoop cluster nodes provide resources for data processing?
>>>>> datanode

6. Which service manages cluster CPU and memory resources?
>>>>> Resource manager

7. How does a NameNode determine DataNode availability?
>>>>> heartbeat

8. What construct is grated CPU and memory resources?

9. What daemon / service is responsible for application fault tolerance?
>>>>> Application Manager

10. By default, the various tasks for a single application / job will run on the same node if sufficeient memory and CPU resources are available on that node
>>>>>  false 

11. Using Hadoop's default settings, how much data will you be able to store on your Hadoop cluster
if it has 12 nodes with 4TB of raw disk space per node allocated to HDFS storage?
>>>>> 12 * 4 / 3(Replication)

12. Which two describe functions of the NodeManager in YARN?
>>>>> Monitoring and reporting container status for map and reduce tasks
>>>>> Monitoring the status of the ApplicationMaster cotainer and restarting on failure

13. You decide to create a cluster which runs HDFS in High Availability mode with automatic failover
using Quorum-based Storage. Which service keeps track of which NameNode is active at any given moment 
>>>>> Zookeeper

14. Identify the function performed by a Secondary NameNode daemon configured to run with a single NameNode
>>>>> it combines the fsimage and edits files produced by the NameNode

15. You have a cluster running 32 slave nodes and three master nodes, running MapReduce v1 You execute the command
hdfs fsck/
(1) the number of datanodes in the cluster
(2) the number of under-replicated blocks in the cluster


******************************************* 몽고db




******************************************* Flume/Kafka 시험
1. What are the 3 main components of a Flume data flow?
>>>> Source, Channel, Sink4


2. What are the Interceptors and how are they used?
>>>> Source로 들어온 이벤트 수정 또는 버릴 때 사용됨. 소스로 들어온 이벤트는 Interceptor 거친 뒤 채널에 전달

3. How do you achieve context routing?
>>>> Interceptors, Channel Selectors

4. What does it mean in kafka that Producers and Consumers are decoupled?
>>>> Producer는 특정 topic의 메시지를 생성한 뒤 해당 메시지를 broker에 전달한다. Broker가 전달받은 메시지를 topic별로 분류하여 쌓아놓으면, 
해당 topic을 구독하는 consumer들이 메시지를 가져가서 처리

5. What does it mean in Kafka that Producers and Consumers are decoupled?
>>>> Kafka 단점 : Flume과 다르게 Producer와 Consumer에 코딩 필요한데, 
Flafka는 Producer와 Consumer를 Flume 컴포넌트 구성을 통해서 사용 편의성을 높일 수 있다.  

6. Which of the following was created to allow you to flow data from a source into your Hadoop?
>>>> (C) Flume 

7. Which among the following is core Components of Flume?
>>>>(A) Flume의 Core Component는 EVENT, AGENT

8. Which tool is best suited to import a portion of a relational database every day as files into HDFS and 
generate java classes to interact with that imported data?
>>>>(D) sqoop

9. Which among the following channel is available in Flume?
>>>>(D) ALL       JDBC/FILE/MEMORY

10. Which among the following is the fastest channel, however, has the risk of data loss?
>>>>(C) MEMORY Channel

11. Which is the reliable channel in Flume to ensure that there is no data losS?
>>>>(B) FILE Channel

12. Which Channel stores the events in an embedded Derby database
>>>> (A) JDBC Channel

13. What option does Sqoop use to import only specific rows in the tables?
>>>> (A) Where

14. Which of the following is NOT a key component of Kafka?
>>>> (D) HDFS     topic, producer, broker

15. what is the role of storing messages in topic in kafka
>>>> Producer : 특정 Topic에 해당하는 메시지를 생성하는 프로세스, 메시지를 Broker에 전달(발행/Publish)
Partition에 분산되어 저장. 각 partition은 0부터 1씩 증가하는 offset 값을 메시지에 부여하는데 이 값은 각 partition내에서 메시지를 식별하는 ID로 사용된다. 
Offset 값은 partition마다 별도로 관리되므로 topic내에서 메시지를 식별할 때는 partition 번호와 offset 값을 함께 사용

16. Which of the following elemets is processed by reading and processing messages in the Kafka Topic?
>>>> (D) Consumer
메시지 수신 API
Broker에게서 구독(Subscribe)하는 Topic의 메시지를 가져와 사용(처리)하는 프로세스
Topic 당 할당된 스레드 개수만큼 스레드가 만들어지면 Partition 으로부터 메세지를 읽음.
하나의 스레드는 1개 이상 partition 으로 부터 메시지를 읽을 수 있다.
public void run 메소드 내 while(is.hasNext())에서 블록킹돼 있다가 파티션으로 메시지가 들어오면 이 곳에서 메시지를 읽는다. 따라서 다른 타겟으로 메시지를 처리하는 데 적합한 장소라고 할 수 있다. 즉 메시지를 파일로 저장하던지 대용량 입력이 가능한 하둡이나 NoSQL로 저장하기에 유용하다.

>>>> Broker
토픽을 기준으로 메시지 관리
broker 는 클러스터로 구성 (이에 대한 분산 처리는 zookeeper 가 처리)
Producers와 Consumers가 만날 수 있도록 메시지를 관리하는 서버 클러스터로 Producer에게서 전달받은 메시지를 Topic별로 분류한다. 
여러대의 Broker Cluster로 구성 가능하며, Zookeeper에 의해 각 노드가 모니터링 된다.


******************************************* Hive/Oozie
1. The impala metastore requires an undelying SQL database
>>>> True 

2. What property is used to specify the block size of a file stored in HDFS
>>>> dfs.block.size Hadoop 버전 1.0 > 64MB, 버전 2.0 >128MB

3. A Hive table consists of a schema stored in the Hive _____ and data stored in ______
>>>> metastore, HDFS

4. Describe the daemons used in the impala(master and worker) and the hive service on Hadoop
>>>> 각각의 WorkNode에서 진행되며 Hive metastore와 공유됨
     master node = state store + catalog server
     impala는 metadata를 캐시를 해서 관리하고 Hive와 달리 Mapreduce를 사용하지 않아 빠른 성능을 보여줌 

5. Which two describe functions of the ResourceManager in YARN
(1)
(2)
(3)
(4)
(5)
(6)
>>>> 3,6

6. Which two daemons typically run on each master node in a Hadoop cluster running MapReduce v2(MRv2) on YARN?
>>>>Resource Manager, jornalnode

7번 중복시험

8. Your cluster has slave-nodes in three different racks, and you have written a rack topology script
identifying each machine as being in rack1, rack2, rack2. A client machine  outside of the cluster
writes a small (one-block) file to HDFS. The first replica of the blockis written to a node on rack2
How is block placement determined for the other two replicas?
>>>> (3)

9. What happens under YARN if a Mapper on one node hangs while running a MapReduce job?  
>>>> (3)

10. Identify the function did not work by a Stand-by NameNode daemon configured to run with a Active NameNode
>>>> (1)

보너스
1. Which command would you use to see the structure(metadata) of a relation 
>>>> DESCRIBE

2. Which command would you use to display the actual data within a relation to your screen
>>>> DUMP

3. if you do't specify type of data being loaded, what will Pig use as a default
>>>> CHARARRAY

4. Which of Pig's built-in functions would you use to capitalize a string
>>>> UPPER

5. What are some reasons you would not relace a typical relational database with Hive or Impala
>>>> 빅데이터에서(수백억)의 정형, 비정형 데이터를 관계형 DB가 스토리지를 늘린다고해서 감당할 수 없기에 대체 할 수 없음

6. What is a difference between an external and a Hive- or Impala-managed table
>>>> Managed Table은 테이블을 drop하면 관리하는 파일도 삭제가 되고, External Table은 파일은 보관된다.


Hive metastore에서 테이블 스키마를 가져와 질의문의 적합성을 판단한다.
HDFS 네임노드에서 질의 수행에 필요한 데이터 블록과 각각의 위치 정보를 수집한다.
최근에 업데이트된 Impala 메타데이터를 기반으로 클러스터 내 모든 impalad에게 질의 수행에 필요한 정보들을 전파한다.
질의와 메타데이터를 전달받은 모든 impalad는 각자 처리할 데이터 블록을 로컬 디렉터리에서 읽어와 질의 처리를 수행한다.
모든 impalad에서 작업이 완료되면 사용자로부터 질의를 받았던 impalad는 결과를 취합해 사용자에게 전달한다.


******************************************* Spark










******************************************* Big Data 아키텍쳐 Test
1. Compare micro-bach vs event driven in streaming processors
>>>>> micro-bach - 계속 들어오는 스트림 데이터를 초 단위로 잘라 처리하는 것
>>>>> event driven - 유입되는 Event들을 스트리밍 시스템에 도착하는 시점에 하나씩 처리

2. What is the main advantage of using Dataframe API compared to RDD Core API
>>>>> Schema를 가진 데이터를 다루기 위한 효율적인 방법을 제공
다양한 데이터 포맷(JSON, Hive table, parquet 등) 
JDBC/ODBC 서버 제공 함으로써 Spark SQL을 통해 SQL로 데이터 질의 가능

3. You are analyzing streaming data. You want to count on a key value over ther last 10 minutes of data, every 2 minutes
Assume that you are batching your data every 1 minutes. What function/method would you use?
>>>>> StreamingContext(sc, 60)
>>>>> CountByWindows(600, 120)

4. What are DataFramed
>>>>> DataFamed란 Spark SQL에서 RDD DataFrame을 이용하여 Schema를 가진 데이터를  다룰 수 있다.

5. What are the services (daemon) utillized for HDFS HA Mode?
>>>>> Active NameNode, stanbyName Node, zkfc, journalnode 와 datanode

6. What is schema evolution in Avro?
>>>>> 효율적인 데이터 Serialization 프레임워크로 데이터는 스키마 정의에 따라 직렬화되며, 스키마 변경 기능을 제공한다 
데이터와 스키마가 같이 저장

7. Why Kudu 
>>>>> 어마한 양의 데이터 Scanning이 빠른 HDFS와 Low Latency의 장점을 가진 HBase의 장점을 사용함

8. What is difference between SRF and DRF in pool management
>>>>> 기본적으로 Yarn어플리케이션이 공평하게 리소스를 할당하는 정책을 갖음
기본적으로 메모리 기반으로 리소스를 나누는데 (SRF), DRF를 적용하면 CPU도 기준잡고 리소스를 배분

9. Issue command(s) to save the Namenode metadata to disk
>>>>> Safe mode에서 hdfs dfsadmin -saveNamespace

10. What is Hdoop client? Issue a command to copy a file in your local linux directory to your home directory in Change the block size to 64MB instead of the default 128MB in a CHD installation
hdfs dfs -D fs.local.blocksize = 64000000 -put localdir hdfsdir












